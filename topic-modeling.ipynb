{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install all libraries and prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Forbidden).)."
     ]
    }
   ],
   "source": [
    "# tensorflow packages\n",
    "%pip install absl-py\n",
    "%pip install tensorflow\n",
    "%pip install tensorflow-hub\n",
    "# seaborn\n",
    "%pip install seaborn\n",
    "# ML\n",
    "%pip install scikit-learn\n",
    "# Facebook's pytorch\n",
    "%pip install torch\n",
    "# Hugging face\n",
    "%pip install transformers datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import pipeline\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# NLP\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some nltk required data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the deep Learning NLP Models (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal sentence encoder (from Google)\n",
    "USE_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Function to use USE encoder\n",
    "def embed(input):\n",
    "    return np.array(USE_encoder(input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment model from hugging face platform\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(input):\n",
    "    result = sentiment_model(input[:512])\n",
    "    sign = 1 if(result[0]['label']==\"POSITIVE\") else -1\n",
    "    value = result[0]['score']\n",
    "    return sign*value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"panekData.tsv\", sep=\"\\t\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit data to reviews we wish to build topics off of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at only the side effects\n",
    "Transcript = df['Transcript'].dropna().tolist()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsTopicModel:\n",
    "\n",
    "    STOPWORDS = stopwords.words('english') # stopwords from ntlk\n",
    "\n",
    "    EMBEDDING_DIM = 512 # USE encoder\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, reviews):\n",
    "        self.X = self.clean(reviews)\n",
    "\n",
    "    # Clean text method\n",
    "    def clean(self, reviews):\n",
    "        # Words to replace\n",
    "        string_map = {'\\r': '', '\\n': '', '/': ' ', \"'\": \"\", '\"': ''}\n",
    "        reviews_cleaned = reviews[:]\n",
    "        for i in range(len(reviews_cleaned)):\n",
    "            for s in string_map:\n",
    "                reviews_cleaned[i] = reviews_cleaned[i].replace(s, string_map[s]).lower() \n",
    "        # Transform the reviews into embeddings dataframe\n",
    "        X = embed(reviews_cleaned)\n",
    "        X = pd.DataFrame(X)\n",
    "        X.index = reviews_cleaned\n",
    "        return X\n",
    "\n",
    "    # Method to determine number of topics (a.k.a kmeans cluster number)\n",
    "    def elbow_plot(self):\n",
    "        cluster_sizes = list(range(1, 81))\n",
    "        cluster_scores = []\n",
    "        for n in cluster_sizes:\n",
    "            kmeans = KMeans(n_clusters=n)\n",
    "            kmeans.fit(self.X)\n",
    "            cluster_scores.append(kmeans.inertia_)\n",
    "        plt.figure()\n",
    "        plt.plot(cluster_sizes, cluster_scores)\n",
    "        plt.show()\n",
    "\n",
    "    # Method to extract topics from text data\n",
    "    def create_topics(self, num_topics):\n",
    "        # Cluster the reviews\n",
    "        kmeans = KMeans(n_clusters=num_topics)\n",
    "        kmeans.fit(self.X)\n",
    "\n",
    "        # Create the final topics dataframe\n",
    "        topics_df = self.X.copy()\n",
    "        topics_df['topic'] = kmeans.labels_.copy()\n",
    "        topics_df['topic'] = topics_df['topic'].astype(int)\n",
    "\n",
    "        # Create summary keywords per topic\n",
    "        topic_keywords = {}\n",
    "        for topic in topics_df['topic'].unique():\n",
    "            topic_reviews = topics_df.query(\"topic == {}\".format(topic)).index.tolist()\n",
    "            topic_centroid = kmeans.cluster_centers_[topic] # cluster center\n",
    "            topic_keywords[topic] = self.get_closest_words(topic_reviews, topic_centroid)\n",
    "        topics_df['topic_keywords'] = topics_df['topic'].map(topic_keywords)\n",
    "\n",
    "        # Score the sentiment of each review\n",
    "        topics_df['sentiment'] = [sentiment(r) for r in topics_df.index.values.tolist()]\n",
    "\n",
    "        # Final result\n",
    "        self.topics_keywords = topic_keywords\n",
    "        self.topics_df = topics_df.copy()[['topic', 'topic_keywords', 'sentiment']]\n",
    "\n",
    "\n",
    "    def get_closest_words(self, reviews, centroid):\n",
    "        word_distances = {}\n",
    "        for r in reviews:\n",
    "            review_words = [w for w in word_tokenize(r) if(w not in self.STOPWORDS)]\n",
    "            for w in review_words:\n",
    "                word_embedding = embed([w]) # return 512 dimensional vector for the word 'w'\n",
    "                # how similar is the word embedding to the topic's centroid (avg of the cluster sample's embeddings)\n",
    "                word_distances[w] = self.cosine_similarity(word_embedding, centroid) # 1 it means match, closer to 0 means different\n",
    "        # done collecting the distances of the words to the topic's center\n",
    "        top_5_keywords = sorted([(word_distances[w], w) for w in word_distances])[-5:]\n",
    "        return \",\".join([x[1] for x in top_5_keywords])\n",
    "\n",
    "    def cosine_similarity(self, x, y):\n",
    "        # x,y are both the appropriate dimension\n",
    "        x = x.reshape(self.EMBEDDING_DIM,)\n",
    "        y = y.reshape(self.EMBEDDING_DIM,)\n",
    "        # calculate cosine similarity\n",
    "        dotproduct = x.dot(y)\n",
    "        x_mag = x.dot(x)**0.5\n",
    "        y_mag = y.dot(y)**0.5\n",
    "        # returns closer to 1 if x and y are similar, closer to 0 if they are different\n",
    "        return dotproduct/(x_mag * y_mag)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = ReviewsTopicModel(Transcript)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create elbow plot to determine optimal number of topics (i.e. clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.elbow_plot() # going to use 40 clusters i.e. 40 topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create final model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.create_topics(num_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topics_keywords[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topics_df.query(\"topic == 10\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at average sentiment per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topics_df.groupby(['topic', 'topic_keywords']).agg({'sentiment': 'mean'}).reset_index()\\\n",
    "    .sort_values(by='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.topics_df.query(\"topic == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
